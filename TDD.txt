Посмотри, возможно в PyOD или ещё где-то твои Mixins или другие фичи уже присутствуют.




Technical Design Document: Universal Anomaly Detection Library
Version: 0.4 (Detailed Specification)
Language: Python 3.9+
Scope: Tabular, Time-Series, Computer Vision, NLP, Graph.
	
	1. Общие положения и цели
Создание унифицированного мета-фреймворка для поиска аномалий, агрегирующего алгоритмы из различных библиотек (Sklearn, PyOD, PyTorch, Transformers, PyG) под единым API.
Архитектурные принципы:
Strict Core, Flexible Input: Ядро навязывает строгий интерфейс методов (fit, predict_score), но допускает полиморфизм входных данных (X) для поддержки сложных модальностей (Текст, Граф).
Backend Agnostic: Библиотека является оберткой. Она не реализует математику с нуля, а управляет жизненным циклом моделей из внешних библиотек.
Unified Output: Гарантия унифицированной метрики. Чем выше скор, тем аномальнее объект.
Modular Dependencies: Ядро остается легковесным. Тяжелые фреймворки загружаются только по требованию.
	
	2. Архитектура Системы
Система реализуется по слоистой архитектуре («Луковица»):
Layer 1: Core (Ядро). Абстрактные классы, контракты данных, исключения, миксины отчетности. Не зависит от тяжелых библиотек.
Layer 1.5: Base Implementations (Шаблоны). Промежуточные абстрактные классы либо реализующие рутину взаимодействия с различными типами бэкендов (статистические, нейросетевые, графовые), но не привязанные к конкретным алгоритмам, либо реализующие рутину взаимодействия с конкретными библиотеками. 
Layer 2: Adapters (Реализация). Обертки (Wrappers) над конкретными внешними алгоритмами. Реализуют логику трансляции параметров и данных.
Layer 3: Infrastructure (Инфраструктура). Реестр алгоритмов (Registry), утилиты валидации и загрузки данных, Lazy Loading.
Layer 4: High-Level API (Фасад). Интерфейс для конечного пользователя (Factory).
	
	3. Спецификация Core API (Layer 1)
Все детекторы наследуются от абстрактного класса BaseDetector.
3.1. Класс BaseDetector
code
Python
from typing import Any, Optional
from abc import ABC, abstractmethod
import numpy as np
import tempfile
import shutil
import os
import json
import joblib

class BaseDetector(ABC):
    def __init__(self, contamination: float = 0.1, **kwargs):
        # Используется для автоматического расчета порога
        self.contamination = contamination
        # Хранилище для внутренней модели (доступ через property)
        self._backend_model = None
        # Вычисленный порог (автоматически определяется в fit)
        self.threshold_: Optional[float] = None
        # Флаг состояния (для Pre-trained моделей может быть True сразу)
        self._is_fitted = False 

    def fit(self, X: Any, y: Optional[Any] = None) -> 'BaseDetector':
        """
        Шаблонный метод обучения.
        """
        # 1. Валидация входных данных (через utils)
        # X = validate_input(X, ...) 
        
        # 2. Фиксация Random Seed (для воспроизводимости)
        self._set_seed()
        
        # 3. Обучение бэкенда (делегируем наследнику)
        self._fit_backend(X, y)
        
        # 4. Автоматический расчет порога
        # Считаем скоры на трейне и берем квантиль
        train_scores = self.predict_score(X)
        self.threshold_ = np.quantile(train_scores, 1 - self.contamination)
        
        self._is_fitted = True
        return self

    @abstractmethod
    def _fit_backend(self, X: Any, y: Optional[Any] = None):
        """Реализация обучения конкретного бэкенда."""
        pass

    @abstractmethod
    def predict_score(self, X: Any) -> np.ndarray:
        """
        Возвращает степень аномальности объекта.
        High value (> threshold) = Anomaly.
        """
        pass

    def predict(self, X: Any, threshold: float = None) -> np.ndarray:
        """
        Бинарная классификация (0 - норма, 1 - аномалия).
        """
        if not self._is_fitted:
            raise RuntimeError("Model is not fitted. Call fit() first.")

        scores = self.predict_score(X)
        
        # Приоритет: Явный аргумент -> Вычисленный порог -> Ошибка
        tr = threshold if threshold is not None else self.threshold_
        
        if tr is None:
            raise ValueError("Threshold not fitted. Run fit() or pass explicit threshold.")
            
        return (scores > tr).astype(int)
    
    @property
    def backend_model(self):
        """Прямой доступ к объекту используемой библиотеки (Low-Level API)."""
        if self._backend_model is None:
            raise RuntimeError("Model is not fitted.")
        return self._backend_model

	# --- СЕРИАЛИЗАЦИЯ (ZIP Container) ---

    def save(self, filepath: str):
        filepath = str(filepath)
        # Убираем расширение, чтобы shutil добавил .zip сам корректно
        base_name = filepath.replace('.zip', '')
        
        with tempfile.TemporaryDirectory() as tmp_dir:
            # 1. Метаданные
            meta = {
                'class_name': self.__class__.__name__,
                'contamination': self.contamination,
                'threshold': self.threshold_
            }
            with open(os.path.join(tmp_dir, "metadata.json"), 'w') as f:
                json.dump(meta, f)
            
            # 2. Бэкенд
            backend_path = os.path.join(tmp_dir, "backend")
            os.makedirs(backend_path)
            self._save_backend(backend_path)
            
            # 3. Атрибуты обертки (безопасно удаляем тяжелый бэкенд из копии)
            state = self.__dict__.copy()
            state.pop('_backend_model', None) 
            joblib.dump(state, os.path.join(tmp_dir, "attributes.pkl"))
            
            # 4. Упаковка в .zip
            shutil.make_archive(base_name, 'zip', tmp_dir)

    def load(self, filepath: str):
        # Обратный процесс: раззиповать -> прочитать JSON -> восстановить атрибуты 
        # -> вызвать _load_backend()

        # Если пользователь забыл .zip, можно попробовать добавить
        if not os.path.exists(filepath) and os.path.exists(filepath + ".zip"):
            filepath += ".zip"

        with tempfile.TemporaryDirectory() as tmp_dir:
            shutil.unpack_archive(filepath, tmp_dir)
            
            # 1. Восстанавливаем атрибуты
            attributes = joblib.load(os.path.join(tmp_dir, "attributes.pkl"))
            self.__dict__.update(attributes)
            
            # 2. Восстанавливаем бэкенд
            backend_path = os.path.join(tmp_dir, "backend")
            self._load_backend(backend_path)
            
            self._is_fitted = True
            return self

    @abstractmethod
    def _save_backend(self, path: str):
        """Адаптер сохраняет свой бэкенд в указанную папку нативными средствами."""
        pass
    
    @abstractmethod
    def _load_backend(self, path: str):
        """Адаптер загружает свой бэкенд из указанной папки."""
        pass

    def _set_seed(self):
        """Установка seed'а (переопределяется или берет из self.random_state)."""
        pass
3.2. Base Implementation Classes (Layer 1.5)
Для соблюдения принципа DRY вводятся промежуточные классы. Их цель — унифицировать поведение схожих по архитектуре бэкендов.
Например:
BaseEstimatorAdapter: Для библиотек с API, похожим на sklearn (fit/score_samples/decision_function). Реализует общий механизм _param_mapping.
BaseDeepLearningAdapter: Для нейросетевых бэкендов. Реализует:
Параметр device ('cpu', 'cuda', 'auto') в __init__.
Автоматический перенос входных тензоров на целевое устройство внутри fit/predict.
Управление DataLoader, batch_size, epochs.
BaseForecastingAdapter: Для алгоритмов Time Series (ARIMA, Prophet). Реализует логику превращения прогноза в скор: Score = |y_true - y_pred|.
3.3. Reporting Interfaces (Mixins)
Дополнительный функционал реализуется через Mixins.
Mixin	Метод	Описание
FeatureImportanceMixin	get_feature_importances()	Глобальная важность признаков.
AnomalyAttributionMixin	predict_contribution(X)	Локальный вклад признаков (Explainability).
ReconstructionMixin	predict_expected(X)	Ожидаемые (восстановленные) данные.
ModelDiagnosticsMixin	get_diagnostics()	Метаданные обучения (Loss curves, stats).
SegmentationMixin	predict_map(X)	Возвращает карту аномальности (N, H, W) для задач CV (сегментация).
3.4. Контракты Данных (Data Contracts)
Каждый Wrapper обязан валидировать входной аргумент X через модуль utils.validation.
Домен (Task)	Ожидаемый формат X	Требования к валидации (Checklist)
Tabular	np.ndarray (N, D)	1. Check Not Empty. 2. Check Finite (No NaN/Inf). 3. Check 2D Shape. 4. Check Sparse (Deny/Allow based on backend).
Time-Series	np.ndarray (Tensor)	1. Check Not Empty. 2. Check Finite. 3. Check 3D Shape (N, T, F).
CV (Image)	np.ndarray / Tensor	1. Check Not Empty. 2. Check Finite. 3. Check 4D Shape (N, C, H, W).
NLP (Text)	List[str]	1. Check Not Empty List. 2. Check Elements are Strings. 3. Check Non-Empty Strings inside list.
Graph	DataObject	1. Check Object has attributes .x and .edge_index. 2. Check Dimensions Match (Nodes count).
Примечание для Time-Series: Если есть внешние факторы (exogenous vars), они включаются в F.
Целевая переменная указывается через параметр target_col в __init__.

	4. Конфигурация и Параметры
Инициализация алгоритмов следует трехуровневой схеме.
4.1. Explicit Standard (Уровень 1: Стандарт)
3-5 самых важных параметров, которые мы стандартизируем во всей библиотеке.
Пример: n_estimators, learning_rate, batch_size.
Цель: Работает автодополнение IDE, жесткая типизация.
4.2. Kwargs (Уровень 2: Проброс)
Параметры, имена которых совпадают с параметрами бэкенда. Пробрасываются "как есть".
Пример: model = RandomForestAdapter(max_depth=5). (Параметр max_depth уйдет в sklearn без изменений).
4.3. Backend Options (Уровень 3: Разрешение конфликтов)
Словарь backend_options используется в двух случаях:
Конфликт имен. Если в нашей библиотеке параметр называется metrics, а в бэкенде тоже metrics, но они значат разные вещи. Через backend_options можно передать "родной" параметр бэкенда, минуя нашу обертку.
Вложенные настройки. Если бэкенд требует словарь параметров (например, optimizer_params={'lr': 1e-3, 'momentum': 0.9}). Передавать это через плоские kwargs неудобно и опасно.
Пример реализации:
code
Python
class MyAdapter(BaseDetector):
    def __init__(self, n_estimators=100, backend_options=None, **kwargs):
        self.n_estimators = n_estimators
        self.backend_options = backend_options or {} # Словарь для сложных случаев
        self.kwargs = kwargs # Простые параметры

    def fit(self, X):
        # Приоритет:
        # 1. backend_options (Самый сильный - прямая указка юзера)
        # 2. kwargs (Проброс)
        # 3. Standard params (Наши дефолты)
        
        final_params = {'n_estimators': self.n_estimators} # Стандарт
        final_params.update(self.kwargs)                   # Добавляем kwargs
        final_params.update(self.backend_options)          # Перезаписываем приоритетом
        
        self._backend_model = BackendModel(**final_params)
        
	5. Инфраструктура (Infrastructure)
5.1. Централизованная Валидация
Модуль mylib.utils.validation:
Функция validate_input(X, task, allow_sparse=False, allow_nan=False).
Выполняет конвертацию (Pandas -> Numpy) и проверки, описанные в Data Contracts.
5.2. Factory & Registry (Dispatcher)
Файл mylib/__init__.py предоставляет единую точку входа get_detector.
Реестр использует Lazy Loading (импорт происходит только в момент запроса алгоритма).
5.3. Reproducibility
Базовый класс реализует метод _set_seed(seed). Обертки обязаны транслировать этот seed в бэкенды (torch.manual_seed, np.random.seed).
5.4. Логирование
Использование logging.getLogger("mylib") вместо print() для вывода предупреждений и прогресса.
5.5. Протокол Сериализации (ModelIO)
Библиотека реализует паттерн Container Storage для сохранения обученных моделей.
Из-за гибридной природы (смесь Python-объектов и C++/CUDA бэкендов) используется формат ZIP-архива (расширение .adl или .zip).
Структура контейнера:
metadata.json: Метаданные (имя класса, версия библиотеки, параметры __init__).
attributes.pkl: Сериализованные атрибуты адаптера (скейлеры, списки признаков), за исключением объекта бэкенда.
backend/: Директория для нативных файлов внешней библиотеки (model.pt, model.joblib).
Требования к реализации:
Core (BaseDetector): Реализует методы save(path) / load(path). Отвечает за создание временной директории, сохранение метаданных и упаковку в ZIP.
Adapters: Реализуют защищенные методы _save_backend(dir_path) и _load_backend(dir_path). Адаптер сохраняет только свою внутреннюю модель, используя нативные инструменты (напр. torch.save), не заботясь об остальной обвязке.

	6. Специфика реализации доменов
6.1. NLP (Text)
Реализация строится на принципе End-to-End Pipeline.
Vectorizer: Wrapper обязан включать векторизатор (TF-IDF или BERT). Пользователь подает сырой текст.
Dimensionality Reduction: Для классических алгоритмов (IForest, LOF) по умолчанию применяется PCA/UMAP после векторизации, чтобы избежать проклятия размерности (768 -> 50 features).
Long Text Handling: Параметр chunking_strategy ('max', 'mean', 'top_k'). Длинный текст режется на куски (512 токенов), а итоговый скор вычисляется как агрегация скоров кусков.
6.2. Graph (GNN)
Backend: Используются обертки над библиотеками типа pygod или dgl.
Data Loader: Так как графы часто хранятся в CSV, библиотека предоставляет утилиту utils.graph_loader.from_csv(nodes_file, edges_file), которая возвращает DataObject.
Limitations: В документации явно указывается поддержка только Inductive или Transductive режимов в зависимости от выбранного алгоритма.
6.3. Computer Vision (CV)
Input: Тензоры (N, C, H, W).
Mixins:
ReconstructionMixin: для Autoencoders (возвращает восстановленное изображение).
SegmentationMixin: для моделей, ищущих дефекты на пиксельном уровне (возвращает тепловую карту).
Viz: Модуль визуализации должен поддерживать наложение тепловых карт (Overlay) на исходное изображение.
6.4. Time-Series (Implementation Details)
Реализация адаптеров для временных рядов должна поддерживать два режима работы через параметр target_cols в __init__:
1) Reconstruction Mode (Default): Если target_cols=None, модель обучается восстанавливать весь входной вектор X.
Аномальность рассчитывается как ошибка реконструкции по всем признакам.
2) Forecasting Mode (with Exog): Если target_cols задан (список индексов), модель использует все признаки из X (включая внешние факторы) для прогнозирования только указанных колонок.
Вход модели: X (все каналы).
Выход модели: Y^ (только целевые каналы).
Скор: Ошибка прогноза |Y^ - X_targets|.

	7. Управление зависимостями
Файлы требований разделены функционально, чтобы пользователь не скачивал лишние гигабайты.
requirements/base.txt:
Назначение: Минимально необходимый набор для работы ядра и простых алгоритмов.
Состав: numpy, scipy (математика), scikit-learn (утилиты валидации и базовые модели).
requirements/deep.txt:
Назначение: Для нейросетевых алгоритмов (Time-Series LSTM, Autoencoders).
Состав: torch, torchvision.
requirements/text.txt:
Назначение: Для работы с текстом (NLP Wrappers).
Состав: transformers (HuggingFace), nltk.
requirements/graph.txt:
Назначение: Для работы с графовыми алгоритмами.
Состав: torch-geometric, pygod (или аналоги).
requirements/viz.txt:
Назначение: Только для модуля визуализации (отрисовка графиков).
Состав: matplotlib, seaborn.
requirements/full.txt:
Назначение: Для разработки и тестов ("Установить всё сразу").
Состав: Ссылки на все вышеперечисленные файлы (-r base.txt, -r deep.txt...).
	
	8. Структура Файлов (Monorepo)
mylib/
├── core/
│   ├── base.py                 # BaseDetector (L1)
│   ├── adapters/               # Layer 1.5 (BaseEstimatorAdapter, etc.)
│   ├── mixins.py               # Reporting Mixins
│   └── exceptions.py           # Custom Exceptions
│
├── algos/                      # Layer 2 (Concrete Wrappers)
│   ├── __init__.py             # Registry Mapping
│   ├── tabular/                # iforest.py, lof.py
│   ├── timeseries/             # lstm_ad.py
│   ├── cv/                     # resnet_ae.py
│   ├── text/                   # tfidf_detector.py
│   └── graph/                  # gnn_detector.py
│
├── utils/                      # Layer 3
│   ├── validation.py           # validate_input()
│   ├── graph_loader.py         # CSV -> DataObject
│   └── seeding.py              # _set_seed logic
│
├── viz/                        # Visualization Module
│   ├── plotting_ts.py          # Графики для временных рядов (сигнал + аномалии)
│   ├── plotting_cv.py          # Графики для CV (наложение масок/heatmap)
│   ├── plotting_graph.py       # Графики графов (networkx visualization)
│   └── utils.py                # Общие утилиты (напр. раскраска матриц)
│
└── __init__.py                 # get_detector (Factory)

	9. Примеры использования (Usage Scenarios)
Этот раздел демонстрирует, как описанные архитектурные принципы реализуются в клиентском коде.
9.1. Базовый сценарий (Tabular + Factory)
Использование единой точки входа и проброс параметров через kwargs.
code
Python
from mylib import get_detector

# 1. Создание (Factory Pattern)
# 'n_estimators' - стандартный параметр библиотеки
# 'n_jobs' - параметр sklearn, пролетает через kwargs
model = get_detector("IsolationForest", n_estimators=100, n_jobs=-1)

# 2. Обучение (Centralized Validation внутри)
model.fit(X_train)

# 3. Предсказание (Unified Output: чем больше, тем аномальнее)
scores = model.predict_score(X_test)
9.2. Продвинутая настройка (Backend Options)
Разрешение конфликтов имен или передача вложенных настроек.
code
Python
# Пример для Torch-based модели
model = get_detector(
    "LSTM", 
    window_size=50,
    # Передача настроек глубоко в бэкенд
    backend_options={
        "optimizer": "AdamW",
        "optimizer_params": {"lr": 1e-4, "weight_decay": 0.01}
    }
)
9.3. NLP Пайплайн (Complex Modality)
Работа с сырым текстом. Векторизация скрыта внутри.
code
Python
logs = [
    "User login successful", 
    "FATAL ERROR: Connection timeout", 
    "User logout"
]

# chunking_strategy='max' позволяет ловить аномалии в длинных логах
model = get_detector("TfidfDetector", chunking_strategy='max')

# fit принимает List[str], валидатор пропускает его
model.fit(logs)
9.4. Графы (Graph Data Container)
Использование утилиты загрузки для формирования правильного объекта данных.
code
Python
from mylib import get_detector
from mylib.utils.graph_loader import from_csv

# 1. Подготовка данных (Utils Layer)
# Превращаем CSV в DataObject(x=..., edge_index=...)
graph_data = from_csv(nodes="users.csv", edges="transactions.csv")

# 2. Модель GNN (обертка над PyGOD)
model = get_detector("GCN")

# 3. fit принимает DataObject
model.fit(graph_data)
9.5. Интерпретация (Mixins)
Использование миксинов для объяснения результата.
code
Python
model = get_detector("LinearModel")
model.fit(X)

scores = model.predict_score(X)

# Проверка возможностей модели через Mixin
from mylib.core.mixins import FeatureImportanceMixin

if isinstance(model, FeatureImportanceMixin):
    # Получаем глобальный вклад признаков
    print(model.get_feature_importances())
    
	10. Definition of Done (Checklist)
При реализации нового алгоритма:

Наследуется от промежуточного класса (напр. BaseSklearnDetector) или реализует валидацию validate_input вручную.

Поддерживает _set_seed.

Импорты тяжелых библиотек — lazy (внутри методов).

Реализует релевантные Mixins (если алгоритм позволяет интерпретацию).

Добавлен в registry.py.



------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Technical Design Document: Universal Anomaly Detection Library
Version: 0.1 (Initial Draft)
Language: Python 3.9+
Scope: Tabular, Time-Series, Computer Vision, NLP, Graph.
1. Общие положения и цели
Создание унифицированного мета-фреймворка для поиска аномалий, агрегирующего алгоритмы из различных библиотек (Sklearn, PyOD, PyTorch, Transformers, PyG) под единым API.
Архитектурные принципы:
Strict Core, Flexible Input: Ядро навязывает строгий интерфейс методов (fit, predict_score), но допускает полиморфизм входных данных (X) для поддержки сложных модальностей (Текст, Граф).
Backend Agnostic: Библиотека является оберткой. Она не реализует математику с нуля, а управляет жизненным циклом моделей из внешних библиотек.
Unified Output: Гарантия унифицированной метрики. Чем выше скор, тем аномальнее объект.
Modular Dependencies: Ядро остается легковесным. Тяжелые фреймворки загружаются только по требованию.
2. Архитектура Системы
Система реализуется по слоистой архитектуре («Луковица»):
Layer 1: Core (Ядро). Абстрактные классы, контракты данных, исключения. Не зависит от тяжелых библиотек.
Layer 2: Adapters (Реализация). Обертки (Wrappers) над внешними алгоритмами. Реализуют логику трансляции параметров и данных.
Layer 3: Infrastructure (Инфраструктура). Реестр алгоритмов (Registry), утилиты загрузки данных, Lazy Loading.
Layer 4: High-Level API (Фасад). Интерфейс для конечного пользователя.
3. Спецификация Core API (Layer 1)
Все детекторы наследуются от абстрактного класса BaseDetector.
Для минимизации дублирования кода вводятся промежуточные абстрактные классы, реализующие стандартные паттерны взаимодействия с популярными бэкендами (BaseSklearnDetector, BasePyODDetector). Реализация конкретных алгоритмов должна наследовать эти классы (обычно) и использовать декларативный стиль (определение _param_mapping и _backend_cls), переопределяя методы только при необходимости нестандартной логики.
3.1. Класс BaseDetector
Типизация входных данных ослаблена до Any для поддержки нетабличных структур (графы, списки строк), но регулируется контрактами данных (см. п. 3.2).
code
Python
from typing import Any, Optional
from abc import ABC, abstractmethod
import numpy as np

class BaseDetector(ABC):
    def __init__(self):
        # Хранилище для внутренней модели (доступ через property)
        self._backend_model = None 

    @abstractmethod
    def fit(self, X: Any, y: Optional[Any] = None) -> 'BaseDetector':
        """
        Обучение модели.
        :param X: Данные для обучения. Формат зависит от домена (см. Data Contracts).
        :param y: Метки (опционально, для Supervised задач).
        :return: self
        """
        pass

    @abstractmethod
    def predict_score(self, X: Any) -> np.ndarray:
        """
        Возвращает степень аномальности объекта.
        :return: Массив (N,). Чем больше значение, тем больше аномалия.
        """
        pass

    def predict(self, X: Any, threshold: float = None) -> np.ndarray:
        """
        Бинарная классификация (0 - норма, 1 - аномалия).
        """
        scores = self.predict_score(X)
        if threshold is None:
            raise ValueError("Threshold must be provided explicitly.")
        return (scores > threshold).astype(int)
    
    @property
    def backend_model(self):
        """Прямой доступ к объекту используемой библиотеки (Low-Level API)."""
        if self._backend_model is None:
            raise RuntimeError("Model is not fitted.")
        return self._backend_model
3.2. Контракты Данных (Data Contracts)
Каждый Wrapper обязан валидировать входной аргумент X согласно своему домену.
Домен (Task)	Ожидаемый формат X	Требования к реализации
Tabular	np.ndarray (N, D)	Классическая матрица признаков. Строки независимы.
Time-Series	np.ndarray (Tensor)	Формат (N_windows, Time_steps, Features). Wrapper может включать логику нарезки окон, если подан плоский ряд.
CV (Image)	np.ndarray / Tensor	Формат (N, Channels, Height, Width).
NLP (Text)	List[str]	Список сырых строк. Wrapper обязан содержать встроенный векторизатор (TF-IDF/Transformer).
Graph	DataObject	Объект-контейнер с атрибутами .x (узлы) и .edge_index (связи). Совместимость с torch_geometric.
4. Конфигурация и Параметры
Инициализация алгоритмов следует паттерну "Explicit Standard + Backend Kwargs".
Явные параметры: В __init__ выносятся 3-5 ключевых параметров, имена которых стандартизированы в библиотеке (например, n_estimators, learning_rate).
Параметры бэкенда: Все остальные параметры передаются через **kwargs или словарь backend_options и пробрасываются в конструктор внутренней библиотеки без изменений.
Пример реализации:
code
Python
class RandomForestAdapter(BaseDetector):
    def __init__(self, n_estimators=100, **kwargs):
        self.n_estimators = n_estimators
        self.backend_kwargs = kwargs # Сохраняем "хвост"

    def fit(self, X: Any, y=None):
        from sklearn.ensemble import IsolationForest
        # Mapping стандартных имен + проброс специфичных
        self._backend_model = IsolationForest(
            n_estimators=self.n_estimators, 
            **self.backend_kwargs
        )
        self._backend_model.fit(X)
        return self
5. Управление зависимостями
5.1. Стратегия импорта (Lazy Loading)
Для обеспечения модульности и скорости запуска:
Запрещены импорты тяжелых библиотек (torch, tensorflow, transformers) на уровне модуля (top-level scope) в папке algos/.
Импорт производится строго внутри методов __init__, fit или фабричных методов.
5.2. Структура Requirements
Зависимости разделены на группы для гибкости установки в разных окружениях (Dev vs Prod).
requirements/base.txt: Минимальный набор (numpy, scipy, scikit-learn).
requirements/deep.txt: Фреймворки глубокого обучения (torch, torchvision).
requirements/text.txt: Библиотеки NLP (transformers, nltk).
requirements/graph.txt: Библиотеки графов (torch-geometric, dgl).
requirements/viz.txt: Визуализация (matplotlib, seaborn).
requirements/full.txt: Агрегирующий файл (включает все вышеперечисленное).
6. Специфика реализации доменов (Implementation Details)
6.1. NLP (Text)
Wrapper для текстовых данных должен реализовывать полный пайплайн "Текст -> Вектор -> Аномалия". Пользователь не должен векторизовать текст вручную.
code
Python
# Пример логики
class TextDetector(BaseDetector):
    def __init__(self):
        self.vectorizer = TfidfVectorizer() # или BERTTokenizer
    
    def fit(self, X: List[str], y=None):
        vectors = self.vectorizer.fit_transform(X) # Препроцессинг внутри
        self._backend_model.fit(vectors)
6.2. Graph (GNN)
Входные данные передаются как DataObject.
Ограничение: Для трансуктивных алгоритмов (поиск аномалий внутри обучающего графа) метод predict может требовать подачи того же объекта графа, на котором производилось обучение.
code
Python
# Пример логики
class GNNAdapter(BaseDetector):
    def fit(self, X: Any, y=None):
        if not hasattr(X, 'edge_index'):
             raise ValueError("Graph input requires 'edge_index'")
        self._backend_model.train(X.x, X.edge_index)
7. Структура Файлов (Monorepo)
Проект организован как монорепозиторий с разделением алгоритмов по типам данных.
code
Text
mylib/
├── core/                       # Layer 1
│   ├── __init__.py
│   ├── base.py                 # BaseDetector (Abstract)
│   ├── exceptions.py           # Custom Errors
│   └── mixins.py               # TimeSeriesMixin, etc.
│
├── algos/                      # Layer 2 (Implementations)
│   ├── __init__.py             # Registry (Lazy Factory)
│   ├── tabular/                # Tabular Wrappers
│   │   ├── iforest.py
│   │   └── lof.py
│   ├── timeseries/             # TS Wrappers
│   │   ├── lstm_ad.py
│   │   └── arima.py
│   ├── cv/                     # Computer Vision
│   │   └── resnet_ae.py
│   ├── text/                   # NLP Wrappers
│   │   ├── bert_ad.py
│   │   └── tfidf.py
│   └── graph/                  # Graph Wrappers
│       └── gcn.py
│
├── utils/                      # Layer 3 (Infra)
│   ├── validation.py           # Проверка форматов данных
│   └── thresholding.py
│
└── viz/                        # Visualization Module
    ├── ts_plot.py
    └── graph_plot.py
8. План разработки (Roadmap)
Этап 1: Фундамент (Core & Tabular)
Реализация BaseDetector с поддержкой Any.
Реализация базовых табличных алгоритмов (IsolationForest, KNN).
Настройка Lazy Imports и системы зависимостей (extras_require).
Этап 2: Временные ряды и Deep Learning
Реализация TimeSeriesMixin (оконные функции).
Интеграция PyTorch.
Реализация LSTM-AD.
Этап 3: Сложные модальности (NLP & Graph)
Реализация text: Адаптеры над Transformers.
Реализация graph: Интеграция с PyTorch Geometric.
Этап 4: Экосистема
Модуль визуализации viz.
High-Level API (Фабрики).
